# -*- coding: utf-8 -*-
"""Module 1- Pandas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RpjtrakHGZTL47hmo3kKP-soJsWCqYhw

#Module 1
   ### Welcome to the Answer notebook for Module 1 !
"""

!pip install -U -q PyDrive

#this notebook is written in google colab

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth 
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once per notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

csv_link= "https://drive.google.com/open?id=1duo34YUjESpncR1ijd-vdsOj7j1HIwdr"
fluff, id= csv_link.split('=')
print(id)

"""---


## Query 1.1
    Import the csv file of the stock of your choosing using 'pd.read_csv()' function into a dataframe. Shares of a company can be offered in more than one category. The category of a stock is indicated in the ‘Series’ column. If the csv file has data on more than one category, the ‘Date’ column will have repeating values. To avoid repetitions in the date, remove all the rows where 'Series' column is NOT 'EQ'. Analyze and understand each column properly. You'd find the head(), tail() and describe() functions to be immensely useful for exploration. You're free to carry out any other exploration of your own.
"""

import pandas as pd
downloaded= drive.CreateFile({'id' : id})
downloaded.GetContentFile('RAYMOND.csv')

#to import the csv files of stocks and save it as a dataframe 
data= pd.read_csv('RAYMOND.csv')

"""**This notebook is written in google Colab thus pydrive authentication is used,  in case of local machine just use    data= pd.read_csv("RAYMOND.csv")**"""

# Commented out IPython magic to ensure Python compatibility.
#import libararies

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import seaborn as sns
# %matplotlib inline
plt.rcParams['figure.figsize'] = (7, 4)
import seaborn as sns

data.head()
#returns top 5 rows of dataframe by dafualt.

print('The data has {0} rows, and {1} columns'.format(data.shape[0], data.shape[1]))

data.describe()
#returns information like mean and standard deviation of each feature

data.info()
#gives info regarding type and no. of entries of each series

data.tail()
#return last 5 rows of dataframe

"""**We have to check if data contains any NULL value before further exploration.**"""

#checking for missing values in the data.

missing_data= data.isnull().sum()/len(data)
missing_data= missing_data[missing_data > 0]

missing_data=missing_data.sort_values()

print(missing_data)

"""**We can see that there is no missing value in any Series of dataframe.**"""

#to check if we have NOT EQ in our Seires column.

data['Series'].unique()
#returns all the unique values in given column of data

"""**We can see that data['Series'] only contain EQ.**

---


## Query 1.2 
    Calculate the maximum, minimum and mean price for the last 90 days. (price=Closing Price unless stated otherwise)

**A technique would be to make a seperate dataframe for last 90 days.**
"""

#to get he date of the last row of our dataframe.
data['Date'][493]

#Last Date : 13 May 2019
#Date 90 days ago : 12 Feb 2019

#to check for the date in dataframe
for i in range (0, 493):
  if data['Date'][i] == '12-Feb-2019':
    
    #prints the index of row with give date.
    print(i)
    
#print date
data['Date'][i]

#create a new data frame data_90_days
#all columns are same but rows are from index 435 to 493 giving last 90 days
data_90_days= data[:][435 : 494]

"""**Now we have created a seperate dataframe for last 90 days, we can use describe( ) to find statistical insights of new data.**"""

data_90_days.describe()

"""**Printing the required outputs for query 1.2**."""

print("For last 90 days : \nThe minimum value of Price {}\nThe maximum value of Price {}\nThe mean value of Price {}"
      .format(data_90_days['Close Price'].min(), data_90_days['Close Price'].max(), data_90_days['Close Price'].mean()))

print('\nprice = Closing Price unless stated otherwise')

"""---


## Query 1.3 
    Analyse the data types for each column of the dataframe. Pandas knows how to deal with dates in an intelligent manner. But to make use of Pandas functionality for dates, you need to ensure that the column is of type 'datetime64(ns)'. Change the date column from 'object' type to 'datetime64(ns)' for future convenience. See what happens if you subtract the minimum value of the date column from the maximum value.
"""

#to check the data types of each column of dataframe
#built in pandas function info( ) can help us.

data.info()

"""**We can see that 9 of our data columns contain floating point value, three of them have integer value and other are of object type.**

**After analysis of data types for each column we can seperate numerical and categorical fetaures.**
"""

#to calculate features with numerical data
numeric_data= data.select_dtypes(include=[np.number])
#to calculate features with non-numerical data
categorical_data= data.select_dtypes(exclude= [np.number])

print('We have {} numeric features and {} categorical features'.format(numeric_data.shape[1], categorical_data.shape[1]))

"""**Now, we are interested to learn about the correlation behavior of numeric variables. Out of 38 variables, I presume some of them must be correlated. If found, we can later remove these correlated variables as they won't provide any useful information to the model.**"""

#to find correlation of numerical data
corr= numeric_data.corr()

print(corr)

"""**Let's say we want to find the correlation of our input factors with Close Price for a day.**"""

#to print top correlated factors 
print('The top correlated factors are\n{}'.format(corr['Close Price'].sort_values(ascending=False)[: 5]))

#to print least correlated factors
print('The least correlated factors are\n{}'.format(corr['Close Price'].sort_values(ascending=False)[-5 :]))

#we can also see if the data is skewed in its probabilty distribution.
data[ : ].skew()

"""**We can plot the data distribution of any data column with help of seaborn plotting library.**"""

sns.distplot(data['Total Traded Quantity'])

"""**We see that the target variable Total Traded Quantity has a right-skewed distribution. We'll need to log transform this variable so that it becomes normally distributed. A normally distributed (or close to normal) target variable helps in better modeling the relationship between target and independent variables.**"""

#now transforming the variable
target = np.log(data['Total Traded Quantity'])
print ('Skewness is', target.skew())
sns.distplot(target)

"""**Now we can see it's more normally distributed, such analysis can be done with any data column.**"""

#to print data type of data["Date"]
print(type(data['Date'][0]))

'''We can see that the data is of object type.
      we will convert it to suitable datatype before further operation.'''

#to convert the elements of date column from type.object to type.datetime64(ns)
data['Date']= pd.to_datetime(data['Date'])

#check the updated data type
type(data['Date'][0])

"""**We converted elements data["Date"] Series to datatime64(ns) type to make use of Pandas functionality for dates.**"""

#subtracting the minimum value of the date column from the maximum value.
print(data['Date'][493] - data['Date'][0])

"""**We can see on subtraction we get the number of days passed during the intervel.**

---


## Query 1.4
    In a separate array , calculate the monthwise VWAP (Volume Weighted Average Price ) of the stock. 
    ( VWAP = sum(price*volume)/sum(volume) ) 
    To know more about VWAP , visit - VWAP definition 
    {Hint : Create a new dataframe column ‘Month’. The values for this column can be derived from the ‘Date” column by using appropriate pandas functions. Similarly, create a column ‘Year’ and initialize it. Then use the 'groupby()' function by month and year. Finally, calculate the vwap value for each month (i.e. for each group created).

**In order to group our dataset by months and years we will first create new columns with month and year for a given data. We have already converted our ['Date'] column to suitable type thus we can get some help from pandas here.**
"""

#initialize a new column and provide 0 value to each of its row.
data['month']= 0

#now for entire length of data we would replce the 0 with month value
#we can get month value from data['Date']
for i in range (0, 494):
  data['month'][i]= data['Date'][i].month

#initialize a new column and provide 0 value to each of its row.
data['year']= 0

#now for entire length of data we would replce the 0 with year value
#we can get year value from data['Date']
for i in range(0, 494):
  data['year'][i]= data['Date'][i].year

data.head()

"""**We can see that month and year are added to our dataset.**



**Now in order to calculate monthwise VWAP (Volume Weighted Average Price ) of the stock. We first need to find out the volume of stocks for each day.**

**The *volume of trade is the total quantity of trading on a given day*. Here it is given by data['Total Traded Quantity']. But since we don't have the exact price of each trade that occurred during the day length, we will be using average price  for given day in order to calculate our VWAP. 
We can see that average price * volume = Turnover.**
"""

data['pr_vol']= (data['Average Price'] * data['Total Traded Quantity'])   #pr_vol contain price * volume for a given day

"""**We can save the daily volume multiplied by daily average price of trade in a new column called pr_vol.**"""

#using pandas group by to group dataframe by month and year simultaneously.
#After grouping monthly summation of each value is calculated.
groups_mm_yyyy=data.groupby([(data['year']),(data['month'])]).sum()

"""**We can see on monthly summation we get sum(Price* Volume)= sum(pr_vol). Which forms up our numerator for calculating VWAP.** 

**So VWAP will be = sum(pr_vol) for each month / sum(Volume or Total Traded Quantity )for each month**
"""

#printing first few rows of new dataframe.
groups_mm_yyyy.head()

"""**Once we have monthy pr_vol and Volume in out new datafram groups_mm_yyyy, grouped by months, can do perform simple divison operation to calculate monthswise VWAP.**"""

#now to calculate VWAP
vwap = groups_mm_yyyy['pr_vol'] / groups_mm_yyyy['Total Traded Quantity']

#printing monthwise vwap.
print(vwap)

"""---


## Query 1.5
    Write a function to calculate the average price over the last N days of the stock price data where N is a user defined parameter. Write a second function to calculate the profit/loss percentage over the last N days.
    Calculate the average price AND the profit/loss percentages over the course of last -
    1 week, 2 weeks, 1 month, 3 months, 6 months and 1 year.
    {Note : Profit/Loss percentage between N days is the percentage change between the closing prices of the 2 days }

**A custom function which takes in number of days as a parameter is written here to return the average value of price over last N days.**

**Where, Average Price over last N days= sum of average prices for last N days / no. of days**
"""

def get_avg_price(days):
  date_value= data['Date'][493] - pd.to_timedelta(days, unit='D')
 
  print("The average price for last {} days is".format(days))
  for i in range(0, 494):
    if date_value > data['Date'][i]:
      data_n_days= data[:][i+1 : 494]
     
      avg_price= (data_n_days['Average Price'].sum() / days)
      
    elif date_value == data['Date'][i]:
      data_n_days = data[:][i : 494]
      
      avg_price= (data_n_days['Average Price'].sum() / days)
      
  return avg_price

"""**Below are calculated average prices over the time period of 1 week, 2 weeks, 1 month, 3 months, 6 months and 1 year**."""

get_avg_price(7)

get_avg_price(14)

get_avg_price(30)

get_avg_price(90)

get_avg_price(180)

get_avg_price(365)

days= int(input("Enter the number of days"))
get_avg_price(days)

"""**A custom function which takes in number of days as a parameter is written here to return the profit/loss percentage of price over last N days.**

**Where, profit/loss percentage over last N days=  percentage change between the closing prices of the 2 days**
"""

def get_pr_loss_perc(days):
  date_value= data['Date'][493] - pd.to_timedelta(days, unit='D')
 
  print("The profit loss percentage for last {} days is".format(days))
  for i in range(0, 494):
    if date_value > data['Date'][i]:
      data_n_days= data[:][i+1 : 494]
     
      pr_loss_change= (data_n_days['Close Price'].pct_change() * 100).fillna(0)
      
    elif date_value == data['Date'][i]:
      data_n_days = data[:][i : 494]
      
      pr_loss_change= (data_n_days['Close Price'].pct_change() * 100).fillna(0)
      
  return pr_loss_change

"""**Below are calculated average prices over the time period of 1 week, 2 weeks, 1 month, 3 months, 6 months and 1 year**. 

**A positive value indicate a profit has been recorded and a negative value indicates a loss.**
"""

get_pr_loss_perc(7)

get_pr_loss_perc(14)

get_pr_loss_perc(30)

get_pr_loss_perc(90)

get_pr_loss_perc(180)

get_pr_loss_perc(365)

days= int(input("Enter the number of days"))
get_pr_loss_perc(days)

"""---


## Query 1.6 
    Add a column 'Day_Perc_Change' where the values are the daily change in percentages i.e. the percentage change between 2 consecutive day's closing prices. Instead of using the basic mathematical formula for computing the same, use 'pct_change()' function provided by Pandas for dataframes. You will note that the first entry of the column will have a ‘Nan’ value. Why does this happen? Either remove the first row, or set the entry to 0 before proceeding.

**We are using pandas function pct_change( ) to calculate percantage change in closing prices.**
"""

#using pct_change on Close Price
data_change= data['Close Price'].pct_change()

data_change.head()

#removing the first row
#data_change = data_change[1: 493]
data_change.head()

#changing NaN to zero
data_change= data_change.fillna(0)
data_change.head()

#we will multiply that value with 100 to get actucal percantage change and replace null values with 0.
data['Day_Perc_Change']= (data['Close Price'].pct_change() * 100).fillna(0)

"""**We can see that Day_Perc_Chnage has been sucessfully added to our original data frame.**"""

data.describe()

"""## Query 1.7 
      Add another column 'Trend' whose values are:
    
    'Slight or No change' for 'Day_Perc_Change' in between -0.5 and 0.5
  
    'Slight positive' for 'Day_Perc_Change' in between 0.5 and 1
  
    'Slight negative' for 'Day_Perc_Change' in between -0.5 and -1
  
    'Positive' for 'Day_Perc_Change' in between 1 and 3
  
    'Negative' for 'Day_Perc_Change' in between -1 and -3
  
    'Among top gainers' for 'Day_Perc_Change' in between 3 and 7
  
    'Among top losers' for 'Day_Perc_Change' in between -3 and -7
  
    'Bull run' for 'Day_Perc_Change' >7
  
    'Bear drop' for 'Day_Perc_Change' <-7
"""

#we can use conditional statement for each of different trend

data['trend']= 'Trend Value'
for i in range(0, 494):
  if (data['Day_Perc_Change'][i]<0.5 and data['Day_Perc_Change'][i] > -0.5):
    data['trend'][i]= 'Slight or No Change'
    
  elif (data['Day_Perc_Change'][i]< 1 and data['Day_Perc_Change'][i] > 0.5):
    data['trend'][i]= 'Slight Positive'
    
  elif (data['Day_Perc_Change'][i]< -0.5 and data['Day_Perc_Change'][i] > -1):
    data['trend'][i]= 'Slight Negative'
  
  elif (data['Day_Perc_Change'][i]< 3 and data['Day_Perc_Change'][i] > 1):
    data['trend'][i]= 'Positive'
  
  elif (data['Day_Perc_Change'][i]< -1 and data['Day_Perc_Change'][i] > -3):
    data['trend'][i]= 'Negative'
  
  elif (data['Day_Perc_Change'][i]< 7 and data['Day_Perc_Change'][i] > 3):
    data['trend'][i]= 'Among Top Gainers'
    
  elif (data['Day_Perc_Change'][i]< -3 and data['Day_Perc_Change'][i] > -7):
    data['trend'][i]= 'Among Top Losers'
  
  elif (data['Day_Perc_Change'][i] > 7):
    data['trend'][i]= 'Bull run'
  
  elif (data['Day_Perc_Change'][i]< -7):
    data['trend'][i]= 'Bear Run'

data.head()

data['trend'].unique()

"""## Query 1.8 
    Find the average and median values of the column 'Total Traded Quantity' for each of the types of 'Trend'.
    {Hint : use 'groupby()' on the 'Trend' column and then calculate the average and median values of the column 'Total Traded Quantity'}

**We will group our data by trend and then we will get the statistical insights from our data.**
"""

#new dataframe
group_by_trend= data.groupby([(data['trend'])])

group_by_trend['Total Traded Quantity'].describe()

print("The value of average of Total Traded Quantity of each Trend \n{}\n\n".format(group_by_trend['Total Traded Quantity'].mean()))
print("The value of median of Total Traded Quantity of each Trend \n{}".format(group_by_trend['Total Traded Quantity'].median()))

"""## Query 1.9 
    SAVE the dataframe with the additional columns computed as a csv file week2.csv. In Module 2, you are going to get familiar with matplotlib, the python module which is used to visualize data.
"""

#using pandas to save our updated dataframe.
week_2_csv= data.to_csv('week2.csv')

from google.colab import files
files.download("week2.csv")