# -*- coding: utf-8 -*-
"""Module 3 -Regression -Beta Calculation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A_hKJ1Pi51BM0E-RSXhcKuU6GCPLWvTW

##Module 3 -

### Solution notebook for module 3
"""

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

"""---


##Query 3.1 

    Import the file 'gold.csv' (you will find this in the intro section to download or in '/Data/gold.csv' if you are using the jupyter notebook), which contains the data of the last 2 years price action of Indian (MCX) gold standard. Explore the dataframe. You'd see 2 unique columns - 'Pred' and 'new'. One of the 2 columns is a linear combination of the OHLC prices with varying coefficients while the other is a polynomial function of the same inputs. Also, one of the 2 columns is partially filled.
    
    -- Using linear regression, find the coefficients of the inputs and using the same trained model, complete the entire column.
    
    -- Also, try to fit the other column as well using a new linear regression model. Check if the predictions are accurate. Mention which column is a linear function and which is polynomial.
    (Hint: Plotting a histogram & distplot helps in recognizing the discrepencies in prediction, if any.)
"""

import pandas as pd
gold= pd.read_csv('GOLD.csv')

gold.head()

"""**we can see the columns gold[''Pred] and gold['new'] in the dataframe above.**

**To find out which one is partially filled we can use pandas info( ) function.**
"""

gold.info()

"""**As we can see above that Pred is the column that is partially filled, we can use the filled portion of this column in training data and make rest of it as testing dataset. The prediction on this test set can then be used to fill the entire column.**"""

#to generate the training data by dropping rows with any empty column.
gold_train= gold.dropna()

#testing data will be generated by taking rows with any empty column.
gold_test= gold[pd.isnull(gold).any(axis= 1)]

#just to check 
print('The training data has {} samples, and testing data has {} samples.'.format(len(gold_train), len(gold_test)))

"""**We can now take our OHLC (open, high, low and close ) as features and both Pred and new as labels to train our linear regression model on.**

**NOTE : the training will be done using the training data we generated earlier.**
"""

#features for our model.
X = gold_train.iloc[:, 1: 5].values

#our label
y = gold_train.iloc[:, 7: 8].values

#jsut to check
print("The features are -- \n {0}\n\nThe labels are -- \n {1}".format(gold_train.iloc[:, 1: 5].head(), gold_train.iloc[:, 7: 8].head()))

"""**So we have set our features to be OHLC and our label is Pred right now.**"""

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

#splitting data in training and validation set
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

# Fitting Simple Linear Regression to the Training set
regressor_pred = LinearRegression()
regressor_pred.fit(X_train, y_train)

# check the score on test set, the output is the the probablity for correct prediction.
print("The accuracy of test set is {}".format(regressor_pred.score(X_val, y_val) * 100))

"""**The accuracy on test set is 100, which mean our LINEAR REGRESSION model is able to predict correctly everytime, thus suggesting a linear mapping between inputs and output.**

**The result also suggest that Pred is the output, that is linear combination of OHLC input.**
"""

#to find the cofficient of the features in trained model.
print('The cofficient of the inputs are {}' .format(regressor_pred.coef_))

#to find the interceptt of the features in trained model.
print('The intercept of the inputs are {}' .format(regressor_pred.intercept_))

"""**We can see that the cofficients are integer numbers thus for linear equation. Thus suggesting that Pred is infact linear combination of inputs.**

 **coef_1 * low + coef_2 * high + coef_3 * open + coef_4 * price(close) + intercept = Pred.**
 
 **The next task would be to complete the entire column using the same trained model, we will use regressor_pred as that was the column that had empty rows.**
"""

#predict the values of Pred for all empty column using gold_test column used earlier.
Pred= regressor_pred.predict(gold_test.iloc[:, 1: 5].values)

#update the rows in gold_test
gold_test['Pred']= Pred

#append gold_test to out original data.
gold [ 411 : ]= gold_test

gold.describe()

"""**We have found correct value of Pred for all the empty rows and updated our dataset.**

**Our next step would be to set new column as a output and find if we have a linear mapping from inputs to output.**
"""

#features for our model.
X = gold_train.iloc[:, 1: 5].values

#our label
y = gold_train.iloc[:, 8 : ].values

#jsut to check
print("The features are -- \n {0}\n\nThe labels are -- \n {1}".format(gold_train.iloc[:, 1: 5].head(), gold_train.iloc[:, 8: ].head()))

#splitting data in training and validation set
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)

# Fitting Simple Linear Regression to the Training set
regressor_new = LinearRegression()
regressor_new.fit(X_train, y_train)

# check the score on test set, the output is the the probablity for correct prediction.
print("The accuracy of test set is {}".format(regressor_new.score(X_val, y_val) * 100))

"""**The accuracy on test set is 99.99.. , which mean our LINEAR REGRESSION model is able to predict correctly ALMOST everytime, however this drop in accuracy suggest that there are some higher powers involved in our equation. Thus this mapping is NOT strictly LINEAR**

**The result also suggest that new is the output, that is NOT a linear combination of OHLC input.**
"""

#to find the cofficient of the features in trained model.
print('The cofficient of the inputs are {}' .format(regressor_new.coef_))

#to find the cofficient of the features in trained model.
print('The cofficient of the inputs are {}' .format(regressor_new.intercept_))

"""**We can see that the cofficients are floating point numbers. Thus suggesting that new is not a linear combination of inputs.**"""

import seaborn as sns
sns.distplot(gold['Pred'])

sns.distplot(gold['new'])

"""**The distplot also suggest that the the data points are evenly distributed in case of Pred, and somewhat skewed in case of new, thus suggesting that the predictions in case of Pred are infact more accurate than in case of new.**

---


##Query3.2 
    Import the stock of your choosing AND the Nifty index. 
    
    Using linear regression (OLS), calculate -
    -- The daily Beta value for the past 3 months. (Daily= Daily returns)
    -- The monthly Beta value. (Monthly= Monthly returns)
    
    
    Refrain from using the (covariance(x,y)/variance(x)) formula. 
    
    Attempt the question using regression.(Regression Reference) 
    Were the Beta values more or less than 1 ? What if it was negative ? 
    Discuss. Include a brief writeup in the bottom of your jupyter notebook with your inferences from the Beta values and regression results.
"""

#import the datasets
Laxmi= pd.read_csv('LAXMIMACH.csv')
nifty50= pd.read_csv('Nifty50.csv')

Laxmi_3_mo= Laxmi[:][432 : 494]
nifty50_3_mo= nifty50[:][432 : 494]

"""**We will get data for last 3 months, on average a quarter is taken to be 63 days longer.**"""

daily_prices = pd.concat([Laxmi_3_mo['Close Price'], nifty50_3_mo['Close']], axis=1)
daily_prices.columns = ['Laxmi', 'nifty50']

"""**We will create a new dataframe called daily_prices with closing prices of LAXMIMACH and our index that is, Nifty50.**

**The next step would be to calculate the daily percentage changes for both the columns and drop all NaN from our updated dataframe.**
"""

daily_returns = daily_prices.pct_change(1)
clean_daily_returns = daily_returns.dropna(axis=0)  # drop first missing row
print(clean_daily_returns.head(10))

"""**Good! Now that we have a clean set of daily returns on LAXMIMACH and Nifty 50. Let’s go ahead and make the Ordinary Least Square (OLS)Regression with Statsmodels.**"""

import statsmodels.api as sm
# split dependent and independent variable
X = clean_daily_returns['nifty50']
y = clean_daily_returns['Laxmi']

# Add a constant to the independent value
X1 = sm.add_constant(X)

# make regression model 
model = sm.OLS(y, X1)

# fit model and print results
results = model.fit()
print(results.summary())

# alternatively scipy linear regression
from scipy import stats
slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)

#the slope of the regression lines denotes the beta.
print(slope)

"""###Interpretation of a Beta result

    A stock with a beta of:
    -> zero indicates no correlation with the chosen benchmark (e.g. NASDAQ index )
    -> one indicates a stock has the same volatility as the market
    -> more than one indicates a stock that’s more volatile than its benchmark
    -> less than one is less volatile than the benchmark
    -> 1.5 is 50% more volatile than the benchmark

**Thus value of beta is 0.6544, As you can see from the summary, the coefficient value for (LAXMIMACH) is 0.6544. This suggest that the Laxmimach have be LESS volatile than Nifty 50 over the course of last 3 months.**

**In this method, we regress the company’s stock returns against the market’s returns. The beta (β) is represented by the slope of the regression line.**

 **stock_return= 0.6543 * market_return + (- 0.0008)**

**Next step would be to do the same for monthly beta values, that is to find out the beta values monthly returns for last 3 years.**

**We would group both our datasets by months and years, to get monthly closing prices for both the data sets and calculate monthly percentage change.**
"""

Laxmi['Date']= pd.to_datetime(Laxmi['Date'])

nifty50['Date']= pd.to_datetime(nifty50['Date'])

#initialize a new column and provide 0 value to each of its row.
Laxmi['month']= 0

#now for entire length of data we would replce the 0 with month value
#we can get month value from data['Date']
for i in range (0, 494):
  Laxmi['month'][i]= Laxmi['Date'][i].month

#initialize a new column and provide 0 value to each of its row.
nifty50['month']= 0

#now for entire length of data we would replce the 0 with month value
#we can get month value from data['Date']
for i in range (0, 494):
  nifty50['month'][i]= nifty50['Date'][i].month

#initialize a new column and provide 0 value to each of its row.
Laxmi['year']= 0

#now for entire length of data we would replce the 0 with month value
#we can get month value from data['Date']
for i in range (0, 494):
  Laxmi['year'][i]= Laxmi['Date'][i].year

#initialize a new column and provide 0 value to each of its row.
nifty50['year']= 0

#now for entire length of data we would replce the 0 with month value
#we can get month value from data['Date']
for i in range (0, 494):
  nifty50['year'][i]= nifty50['Date'][i].year

Laxmi.head()

#to group both the datasets by months and years.
Laxmi=Laxmi.groupby([(Laxmi['year']),(Laxmi['month'])]).sum()

nifty50=nifty50.groupby([(nifty50['year']),(nifty50['month'])]).sum()

nifty50.head()

#make a new dataframe with closing prices for both the datasets.
monthly_prices = pd.concat([Laxmi['Close Price'], nifty50['Close']], axis=1)
monthly_prices.columns = ['Laxmi', 'nifty50']

monthly_prices.head(5)

#calculate percentage changes in monthly prices for both the datasets.
#don't forget to drop all the null values from the new dataframe.
monthly_returns = monthly_prices.pct_change(1)
clean_monthly_returns = monthly_returns.dropna(axis=0) *100  # drop first missing row
print(clean_monthly_returns.head(5))

"""**Good! Now that we have a clean set of monthly returns on LAXMIMACH and Nifty 50. Let’s go ahead and make the Ordinary Least Square (OLS)Regression with Statsmodels.**"""

import statsmodels.api as sm
# split dependent and independent variable
X = clean_monthly_returns['nifty50']
y = clean_monthly_returns['Laxmi']

# Add a constant to the independent value
X1 = sm.add_constant(X)

# make regression model 
model = sm.OLS(y, X1)

# fit model and print results
results = model.fit()
print(results.summary())

# alternatively scipy linear regression
from scipy import stats
slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)

print(slope)

"""**As you can see from the summary, the coefficient value for (LAXMIMACH) is 1.1635, which is also confirmed by the slope of our linear mapping.**

**Thus we can infer that our beta value for our monthly returns over the last 3 years is greater than 1, thus suggesting thet our stock i.e. LAXMIMACH is MORE volatile than it's benchmark i.e. Nifty 50.**

**In this method, we regress the company’s stock returns against the market’s returns. The beta (β) is represented by the slope of the regression line.**

 **stock_return= 1.1617 * market_return + (0.7036)**
 
**High-beta stocks are supposed to be riskier but provide a potential for higher returns; low-beta stocks pose less risk but also lower returns, we can understand that for last 3 months our stocks is infact less volatile than market index, we can confirm this by seeing the trend in our stocks for last 3 months, but over the last 3 years our stock has shown higher volitality over the monthly returns and thus offer high returns at the cost of high risk.**
"""